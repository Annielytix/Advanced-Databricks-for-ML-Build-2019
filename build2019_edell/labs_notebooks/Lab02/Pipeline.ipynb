{"cells":[{"cell_type":"markdown","source":["## Creating a Pipeline\n\nIn this exercise, you will implement a pipeline that includes multiple stages of *transformers* and *estimators* to prepare features and train a classification model. The resulting trained *PipelineModel* can then be used as a transformer to predict whether or not a flight will be late.\n\n### Import Spark SQL and Spark ML Libraries\n\nFirst, import the libraries you will need:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Load Source Data\nThe data for this exercise is provided as a CSV file containing details of flights. The data includes specific characteristics (or *features*) for each flight, as well as a column indicating whether or not the flight was late.\n\nYou will load this data into a dataframe and display it."],"metadata":{}},{"cell_type":"code","source":["flightSchema = StructType([\n  StructField(\"DayofMonth\", IntegerType(), False),\n  StructField(\"DayOfWeek\", IntegerType(), False),\n  StructField(\"Carrier\", StringType(), False),\n  StructField(\"OriginAirportID\", StringType(), False),\n  StructField(\"DestAirportID\", StringType(), False),\n  StructField(\"DepDelay\", IntegerType(), False),\n  StructField(\"ArrDelay\", IntegerType(), False),\n  StructField(\"Late\", IntegerType(), False),\n])\n\ndata = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/flights.csv', schema=flightSchema, header=True)\ndata.show()"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Split the Data\nIt is common practice when building supervised machine learning models to split the source data, using some of it to train the model and reserving some to test the trained model. In this exercise, you will use 70% of the data for training, and reserve 30% for testing."],"metadata":{}},{"cell_type":"code","source":["splits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1]\ntrain_rows = train.count()\ntest_rows = test.count()\nprint (\"Training Rows:\", train_rows, \" Testing Rows:\", test_rows)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Define the Pipeline\nA predictive model often requires multiple stages of feature preparation. For example, it is common when using some algorithms to distingish between continuous features (which have a calculable numeric value) and categorical features (which are numeric representations of discrete categories). It is also common to *normalize* continuous numeric features to use a common scale - for example, by scaling all numbers to a proportional decimal value between 0 and 1 (strictly speaking, it only really makes sense to do this when you have multiple numeric columns - normalizing them all to similar scales prevents a feature with particularly large values from dominating the training of the model - in this case, we only have one non-categorical numeric feature; but I've included this so you can see how it's done!).\n\nA pipeline consists of a a series of *transformer* and *estimator* stages that typically prepare a dataframe for\nmodeling and then train a predictive model. In this case, you will create a pipeline with seven stages:\n- A **StringIndexer** estimator for each categorical variable to generate numeric indexes for categorical features\n- A **VectorAssembler** that creates a vector of continuous numeric features\n- A **MinMaxScaler** that normalizes vector of numeric features\n- A **VectorAssembler** that creates a vector of categorical and continuous features\n- A **LogisticRegression** algorithm that trains a classification model."],"metadata":{}},{"cell_type":"code","source":["monthdayIndexer = StringIndexer(inputCol=\"DayofMonth\", outputCol=\"DayofMonthIdx\")\nweekdayIndexer = StringIndexer(inputCol=\"DayOfWeek\", outputCol=\"DayOfWeekIdx\")\ncarrierIndexer = StringIndexer(inputCol=\"Carrier\", outputCol=\"CarrierIdx\")\noriginIndexer = StringIndexer(inputCol=\"OriginAirportID\", outputCol=\"OriginAirportIdx\")\ndestIndexer = StringIndexer(inputCol=\"DestAirportID\", outputCol=\"DestAirportIdx\")\nnumVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normNums\")\nfeatVect = VectorAssembler(inputCols=[\"DayofMonthIdx\", \"DayOfWeekIdx\", \"CarrierIdx\", \"OriginAirportIdx\", \"DestAirportIdx\", \"normNums\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"Late\", featuresCol=\"features\")\npipeline = Pipeline(stages=[monthdayIndexer, weekdayIndexer, carrierIndexer, originIndexer, destIndexer, numVect, minMax, featVect, lr])"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Run the Pipeline as an Estimator\nThe pipeline itself is an estimator, and so it has a **fit** method that you can call to run the pipeline on a specified dataframe. In this case, you will run the pipeline on the training data to train a model."],"metadata":{}},{"cell_type":"code","source":["piplineModel = pipeline.fit(train)\nprint (\"Pipeline complete!\")"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Test the Pipeline Model\nThe model produced by the pipeline is a transformer that will apply all of the stages in the pipeline to a specified dataframe and apply the trained model to generate predictions. In this case, you will transform the **test** dataframe using the pipeline to generate label predictions."],"metadata":{}},{"cell_type":"code","source":["prediction = piplineModel.transform(test)\npredicted = prediction.select(\"features\", col(\"prediction\").cast(\"Int\"), col(\"Late\").alias(\"trueLabel\"))\npredicted.show(100, truncate=False)"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["The resulting dataframe is produced by applying all of the transformations in the pipline to the test data. The **prediction** column contains the predicted value for the label, and the **trueLabel** column contains the actual known value from the testing data."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"PySpark","name":"pysparkkernel","language":""},"language_info":{"mimetype":"text/x-python","pygments_lexer":"python2","name":"pyspark","codemirror_mode":{"version":"2","name":"python"}},"name":"Python Pipeline","notebookId":374219277805845},"nbformat":4,"nbformat_minor":0}
