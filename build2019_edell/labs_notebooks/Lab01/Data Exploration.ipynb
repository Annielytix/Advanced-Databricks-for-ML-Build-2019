{"cells":[{"cell_type":"markdown","source":["## Exploring Data with Dataframes and Spark SQL\nIn this exercise, you will explore data using the Spark Dataframe API and Spark SQL."],"metadata":{}},{"cell_type":"markdown","source":["### Load Data Using an Explicit Schema\nNow you can load the data into a dataframe. If the structure of the data is known ahead of time, you can explicitly specify the schema for the dataframe.\n\nModify the code below to reflect your Azure blob storage account name, and then click the &#9658; button at the top right of the cell to run it."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nflightSchema = StructType([\n  StructField(\"DayofMonth\", IntegerType(), False),\n  StructField(\"DayOfWeek\", IntegerType(), False),\n  StructField(\"Carrier\", StringType(), False),\n  StructField(\"OriginAirportID\", IntegerType(), False),\n  StructField(\"DestAirportID\", IntegerType(), False),\n  StructField(\"DepDelay\", IntegerType(), False),\n  StructField(\"ArrDelay\", IntegerType(), False),\n])\n\nflights = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/raw-flight-data.csv', schema=flightSchema, header=True)\nflights.show()"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Infer a Data Schema\nIf the structure of the data source is unknown, you can have Spark automatically infer the schema.\n\nIn this case, you will load data about airports without knowing the schema.\n\nModify the code below to reflect your Azure blob storage account name, and then run the cell."],"metadata":{}},{"cell_type":"code","source":["airports = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/airports.csv', header=True, inferSchema=True)\nairports.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Use Dataframe Methods\nSpark DataFrames provide functions that you can use to extract and manipulate data. For example, you can use the **select** function to return a new dataframe containing columns selected from an existing dataframe."],"metadata":{}},{"cell_type":"code","source":["cities = airports.select(\"city\", \"name\")\ncities.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Combine Operations\nYou can combine functions in a single statement to perform multiple operations on a dataframe. In this case, you will use the **join** function to combine the **flights** and **airports** dataframes, and then use the **groupBy** and **count** functions to return the number of flights from each airport."],"metadata":{}},{"cell_type":"code","source":["flightsByOrigin = flights.join(airports, flights.OriginAirportID == airports.airport_id).groupBy(\"city\").count()\nflightsByOrigin.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Count the Rows in a Dataframe\nNow that you're familiar with working with dataframes, a key task when building predictive solutions is to explore the data, determing statistics that will help you understand the data before building predictive models. For example, how many rows of flight data do you actually have?"],"metadata":{}},{"cell_type":"code","source":["flights.count()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Determine the Presence of Duplicates\nThe data you have to work with won't always be perfect - often you'll want to *clean* the data; for example to detect and remove duplicates that might affect your model. You can use the **dropDuplicates** function to create a new dataframe with the duplicates removed, enabling you to determine how many rows are duplicates of other rows."],"metadata":{}},{"cell_type":"code","source":["flights.count() - flights.dropDuplicates().count()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Identify Missing Values\nAs well as determining if duplicates exist in your data, you should detect missing values, and either remove rows containing missing data or replace the missing values with a suitable relacement. The **dropna** function creates a dataframe with any rows containing missing data removed - you can specify a subset of columns, and whether the row should be removed in *any* or *all* values are missing. You can then use this new dataframe to determine how many rows contain missing values."],"metadata":{}},{"cell_type":"code","source":["flights.count() - flights.dropDuplicates().dropna(how=\"any\", subset=[\"ArrDelay\", \"DepDelay\"]).count()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### Clean the Data\nNow that you've identified that there are duplicates and missing values, you can clean the data by removing the duplicates and replacing the missing values. The **fillna** function replaces missing values with a specified replacement value. In this case, you'll remove all duplicate rows and replace missing **ArrDelay** and **DepDelay** values with **0**."],"metadata":{}},{"cell_type":"code","source":["data=flights.dropDuplicates().fillna(value=0, subset=[\"ArrDelay\", \"DepDelay\"])\ndata.count()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Explore the Data\nNow that you've cleaned the data, you can start to explore it and perform some basic analysis. Let's start by examining the lateness of a flight. The dataset includes the **ArrDelay** field, which tells you how many minutes behind schedule a flight arrived. However, if a flight is only a few minutes behind schedule, you might not consider it *late*. Let's make our definition of lateness such that flights that arrive within 25 minutes of their scheduled arrival time are considered on-time, but any flights that are more than 25 minutes behind schedule are classified as *late*. We'll add a column to indicate this classification:"],"metadata":{}},{"cell_type":"code","source":["data = data.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\",\"DestAirportID\",\n                   \"DepDelay\", \"ArrDelay\", ((col(\"ArrDelay\") > 25).cast(\"Int\").alias(\"Late\")))\ndata.show()"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Explore Summary Statistics and Data Distribution\nPredictive modeling is based on statistics and probability, so we should take a look at the summary statistics for the columns in our data. The **describe** function returns a dataframe containing the **count**, **mean**, **standard deviation**, **minimum**, and **maximum** values for each numeric column."],"metadata":{}},{"cell_type":"code","source":["data.describe().show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["The *DayofMonth* is a value between 1 and 31, and the mean is around halfway between these values; which seems about right. The same is true for the *DayofWeek* which is a value between 1 and 7. *Carrier* is a string, so there are no numeric statistics; and we can ignore the statistics for the airport IDs - they're just unique identifiers for the airports, not actually numeric values. The departure and arrival delays range between 63 or 94 minutes ahead of schedule, and over 1,800 minutes behind schedule. The means are much closer to zero than this, and the standard deviation is quite large; so there's quite a bit of variance in the delays. The *Late* indicator is a 1 or a 0, but the mean is very close to 0; which implies that there significantly fewer late flights than non-late flights.\n\nLet's verify that assumption by creating a table and using the **Spark SQL** API to run a SQL statement that counts the number of late and non-late flights:"],"metadata":{}},{"cell_type":"code","source":["data.createOrReplaceTempView(\"flightData\")\nspark.sql(\"SELECT Late, COUNT(*) AS Count FROM flightData GROUP BY Late\").show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Yes, it looks like there are significantly more non-late flights than late ones - we can see this more clearly with a visualization, so let's use the inline **%sql** magic to query the table and bring back some results we can display as a chart:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM flightData"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["The query returns a table of data containing the first 1000 rows, which should be a big enough sample for us to explore. To see the distribution of *Late* classes (1 for late, 0 for on-time), in the visualization drop-down list under the table above, click **Bar**. Then click **Plot Options** and configure the visualization like this:\n- **Keys**: Late\n- **Series Groupings**: *none*\n- **Values**: &lt;id&gt;\n- **Aggregation**: Count\n- **Display type**: Bar chart\n- **Grouped**: Selected\n\nYou should be able to see that the sample includes significantly more on-time flights than late ones. This indicates that the dataset is *imbalanced*; which might adversely affect the accuracy of any machine learning model we train from this data.\n\nAdditionally, you observed earlier that there are some extremely high **DepDelay** and **ArrDelay** values that might be skewing the distribution of the data disproportionately because of a few *outliers*. Let's visualize the distribution of these columns to explore this. Change the **Plot Options** settings as follows:\n- **Keys**: *none*\n- **Series Groupings**: *none*\n- **Values**: DepDelay\n- **Aggregation**: Count\n- **Display Type**: Histogram plot\n- **Number of bins**: 20\n\nYou can drag the handle at the bottom right of the visualization to resize it. Note that the data is skewed such that most flights have a **DepDelay** value within 100 or so minutes of 0. However, there are a few flights with extremely high delays. Another way to view this distribution is a *box plot*. Change the **Plot Options** as follows:\n- **Keys**: *none*\n- **Series Groupings**: *none*\n- **Values**: DepDelay\n- **Aggregation**: Count\n- **Display Type**: Box plot\n\nThe box plot consists of a box with a line indicating the median departure delay, and *whiskers* extending from the box to show the first and fourth quartiles of the data, with statistical *outliers* shown as small circles. This confirms the extremely skewed distribution of **DepDelay** values seen in the histogram (and if you care to check, you'll find that the **ArrDelay** column has a similar distribution).\n\nLet's address the outliers and imbalanced classes in our data by removing rows with extreme delay values, and *undersampling* the more common on-time flights:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import rand\n\n# Remove outliers - let's make the cut-off 150 minutes.\ndata = data.filter(\"DepDelay < 150 AND ArrDelay < 150\")\n\n# Separate the late and on-time flights\npos = data.filter(\"Late = 1\")\nneg = data.filter(\"Late = 0\")\n\n# undersample the most prevalent class to get a roughly even distribution\nposCount = pos.count()\nnegCount = neg.count()\nif posCount > negCount:\n  pos = pos.sample(True, negCount/(negCount + posCount))\nelse:\n  neg = neg.sample(True, posCount/(negCount + posCount))\n  \n# shuffle into random order (so a sample of the first 1000 has a mix of classes)\ndata = neg.union(pos).orderBy(rand())\n\n# Replace the temporary table so we can query and visualize the balanced dataset\ndata.createOrReplaceTempView(\"flightData\")\n\n# Show the statistics\ndata.describe().show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Now the maximums for the **DepDelay** and **ArrDelay** are clipped at under 150, and the mean value for the binary *Late* class is nearer 0.5; indicating a more or less even number of each class. We removed some data to accomplish this balancing act, but there are still a substantial number of rows for us to train a machine learning model with, and now the data is more balanced. Let's visualize the data again to confirm this:"],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM flightData"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Display the data as a bar chart to compare the distribution of the **Late** classes as you did previously. There should now be a more or less even number of each class. Then visualize the **DepDelay** field as a histogram and as a box plot to verify that the distribution, while still skewed, has fewer outliers."],"metadata":{}},{"cell_type":"markdown","source":["### Explore Relationships in the Data\nPredictive modeling is largely based on statistical relationships between fields in the data. To design a good model, you need to understand how the data points relate to one another.\n\nA common way to start exploring relationships is to create visualizations that compare two or more data values. For example, modify the **Plot Options** of the chart above to compare the arrival delays for each carrier:\n- **Keys**: Carrier\n- **Series Groupings**: *none*\n- **Values**: ArrDelay\n- **Aggregation**: Count\n- **Display Type**: Box plot\n\nYou may need to resize the plot to see the data clearly, but it should show that the median delay, and the distribution of delays varies by carrier; with some carriers having a higher median delay than others. The same is true for other features, such as the day of the week and the destination airport. You may already suspect that there's likely to be a relationship between delarture delay and arrival delay, so let's examine that next. Change the **Plot Options** as follows:\n- **Keys**: None\n- **Series Groupings**: *none*\n- **Values**: ArrDelay, DepDelay\n- **Aggregation**: Count\n- **Display Type**: Scatter plot\n- **Show LOESS**: Selected\n\nThe scatter plot shows the departure delay and corresponding arrival delay for each flight as a point in a two dimensional space. Note that the points form a diagonal line, which indicates a strong linear relationship between departure delay and arrival delay. This linear relationship shows a *correlation* between these two values, which we can measure statistically. The **corr** function calculates a correlation value between -1 and 1, indicating the strength of correlation between two fields. A strong positive correlation (near 1) indicates that high values for one column are often found with high values for the other, which a strong negative correlation (near -1) indicates that *low* values for one column are often found with *high* values for the other. A correlation near 0 indicates little apparent relationship between the fields."],"metadata":{}},{"cell_type":"code","source":["data.corr(\"DepDelay\", \"ArrDelay\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["In this notebook we've cleaned the flight data, and explored it to identify some potential relationships between features of the flights and their lateness."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"Python Data Exploration","notebookId":4101983005670620},"nbformat":4,"nbformat_minor":0}
