{"cells":[{"cell_type":"markdown","source":["## Evaluating a Classification Model\n\nIn this exercise, you will create a pipeline for a classification model, and then apply commonly used metrics to evaluate the resulting classifier.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load the source data\nflightSchema = StructType([\n  StructField(\"DayofMonth\", IntegerType(), False),\n  StructField(\"DayOfWeek\", IntegerType(), False),\n  StructField(\"Carrier\", StringType(), False),\n  StructField(\"OriginAirportID\", StringType(), False),\n  StructField(\"DestAirportID\", StringType(), False),\n  StructField(\"DepDelay\", IntegerType(), False),\n  StructField(\"ArrDelay\", IntegerType(), False),\n  StructField(\"Late\", IntegerType(), False),\n])\n\ndata = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/flights.csv', schema=flightSchema, header=True)\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1]"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Define the Pipeline and Train the Model\nNow define a pipeline that creates a feature vector and trains a classification model"],"metadata":{}},{"cell_type":"code","source":["monthdayIndexer = StringIndexer(inputCol=\"DayofMonth\", outputCol=\"DayofMonthIdx\")\nweekdayIndexer = StringIndexer(inputCol=\"DayOfWeek\", outputCol=\"DayOfWeekIdx\")\ncarrierIndexer = StringIndexer(inputCol=\"Carrier\", outputCol=\"CarrierIdx\")\noriginIndexer = StringIndexer(inputCol=\"OriginAirportID\", outputCol=\"OriginAirportIdx\")\ndestIndexer = StringIndexer(inputCol=\"DestAirportID\", outputCol=\"DestAirportIdx\")\nnumVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normNums\")\nfeatVect = VectorAssembler(inputCols=[\"DayofMonthIdx\", \"DayOfWeekIdx\", \"CarrierIdx\", \"OriginAirportIdx\", \"DestAirportIdx\", \"normNums\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"Late\", featuresCol=\"features\")\npipeline = Pipeline(stages=[monthdayIndexer, weekdayIndexer, carrierIndexer, originIndexer, destIndexer, numVect, minMax, featVect, lr])\nmodel = pipeline.fit(train)"],"metadata":{"scrolled":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["prediction = model.transform(test)\npredicted = prediction.select(\"features\", col(\"prediction\").cast(\"Int\"), col(\"Late\").alias(\"trueLabel\"))\npredicted.show(100, truncate=False)"],"metadata":{"scrolled":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND truelabel == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND truelabel == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### View the Raw Prediction and Probability\nThe prediction is based on a raw prediction score that describes a labelled point in a logistic function. This raw prediction is then converted to a predicted label of 0 or 1 based on a probability vector that indicates the confidence for each possible label value (in this case, 0 and 1). The value with the highest confidence is selected as the prediction."],"metadata":{}},{"cell_type":"code","source":["prediction.select(\"rawPrediction\", \"probability\", \"prediction\", col(\"Late\").alias(\"trueLabel\")).show(100, truncate=False)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Note that the results include rows where the probability for 0 (the first value in the **probability** vector) is only slightly higher than the probability for 1 (the second value in the **probability** vector). The default *discrimination threshold* (the boundary that decides whether a probability is predicted as a 1 or a 0) is set to 0.5; so the prediction with the highest probability is always used, no matter how close to the threshold."],"metadata":{}},{"cell_type":"markdown","source":["### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a *received operator characteristic (ROC) curve* for the model. The **spark.ml** library includes a **BinaryClassificationEvaluator** class that you can use to compute this. A ROC curve plots the True Positive and False Positive rates for varying threshold values (the probability value over which a class label is predicted). The area under this curve gives an overall indication of the models accuracy as a value between 0 and 1. A value under 0.5 means that a binary classification model (which predicts one of two possible labels) is no better at predicting the right class than a random 50/50 guess."],"metadata":{}},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol=\"Late\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\nauc = evaluator.evaluate(prediction)\nprint (\"AUC = \", auc)"],"metadata":{},"outputs":[],"execution_count":13}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":"3"},"version":"3.6.5","nbconvert_exporter":"python","file_extension":".py"},"name":"Python Classification Evaluation","notebookId":374219277805893},"nbformat":4,"nbformat_minor":0}
