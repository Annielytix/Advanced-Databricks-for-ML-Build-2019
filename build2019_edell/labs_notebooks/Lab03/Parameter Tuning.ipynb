{"cells":[{"cell_type":"markdown","source":["## Tuning Model Parameters\n\nIn this exercise, you will optimise the parameters for a classification model.\n\n### Prepare the Data\n\nFirst, import the libraries you will need and prepare the training and test data:"],"metadata":{}},{"cell_type":"code","source":["# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, MinMaxScaler\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n\n# Load the source data\nflightSchema = StructType([\n  StructField(\"DayofMonth\", IntegerType(), False),\n  StructField(\"DayOfWeek\", IntegerType(), False),\n  StructField(\"Carrier\", StringType(), False),\n  StructField(\"OriginAirportID\", StringType(), False),\n  StructField(\"DestAirportID\", StringType(), False),\n  StructField(\"DepDelay\", IntegerType(), False),\n  StructField(\"ArrDelay\", IntegerType(), False),\n  StructField(\"Late\", IntegerType(), False),\n])\n\ndata = spark.read.csv('wasb://spark@<YOUR_ACCOUNT>.blob.core.windows.net/data/flights.csv', schema=flightSchema, header=True)\ndata = data.select(\"DayofMonth\", \"DayOfWeek\", \"Carrier\", \"OriginAirportID\", \"DestAirportID\", \"DepDelay\", col(\"Late\").alias(\"label\"))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1]"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a classification model"],"metadata":{}},{"cell_type":"code","source":["# Define the pipeline\nmonthdayIndexer = StringIndexer(inputCol=\"DayofMonth\", outputCol=\"DayofMonthIdx\")\nweekdayIndexer = StringIndexer(inputCol=\"DayOfWeek\", outputCol=\"DayOfWeekIdx\")\ncarrierIndexer = StringIndexer(inputCol=\"Carrier\", outputCol=\"CarrierIdx\")\noriginIndexer = StringIndexer(inputCol=\"OriginAirportID\", outputCol=\"OriginAirportIdx\")\ndestIndexer = StringIndexer(inputCol=\"DestAirportID\", outputCol=\"DestAirportIdx\")\nnumVect = VectorAssembler(inputCols = [\"DepDelay\"], outputCol=\"numFeatures\")\nminMax = MinMaxScaler(inputCol = numVect.getOutputCol(), outputCol=\"normNums\")\nfeatVect = VectorAssembler(inputCols=[\"DayofMonthIdx\", \"DayOfWeekIdx\", \"CarrierIdx\", \"OriginAirportIdx\", \"DestAirportIdx\", \"normNums\"], outputCol=\"features\")\nlr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\npipeline = Pipeline(stages=[monthdayIndexer, weekdayIndexer, carrierIndexer, originIndexer, destIndexer, numVect, minMax, featVect, lr])"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. A simple way to do this is to use  **TrainValidationSplit** to evaluate each combination of parameters defined in a **ParameterGrid** against a subset of the training data in order to find the best performing parameters."],"metadata":{}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01, 0.001]).addGrid(lr.maxIter, [10, 5, 2]).build()\ntvs = TrainValidationSplit(estimator=pipeline, evaluator=BinaryClassificationEvaluator(), estimatorParamMaps=paramGrid, trainRatio=0.8)\n\nmodel = tvs.fit(train)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["prediction = model.transform(test)\npredicted = prediction.select(\"features\", \"prediction\", \"probability\", \"label\")\npredicted.show(100)"],"metadata":{"scrolled":false,"collapsed":false},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Compute Confusion Matrix Metrics\nClassifiers are typically evaluated by creating a *confusion matrix*, which indicates the number of:\n- True Positives\n- True Negatives\n- False Positives\n- False Negatives\n\nFrom these core measures, other evaluation metrics such as *precision* and *recall* can be calculated."],"metadata":{}},{"cell_type":"code","source":["tp = float(predicted.filter(\"prediction == 1.0 AND label == 1\").count())\nfp = float(predicted.filter(\"prediction == 1.0 AND label == 0\").count())\ntn = float(predicted.filter(\"prediction == 0.0 AND label == 0\").count())\nfn = float(predicted.filter(\"prediction == 0.0 AND label == 1\").count())\nmetrics = spark.createDataFrame([\n (\"TP\", tp),\n (\"FP\", fp),\n (\"TN\", tn),\n (\"FN\", fn),\n (\"Precision\", tp / (tp + fp)),\n (\"Recall\", tp / (tp + fn))],[\"metric\", \"value\"])\nmetrics.show()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### Review the Area Under ROC\nAnother way to assess the performance of a classification model is to measure the area under a ROC curve for the model. the spark.ml library includes a **BinaryClassificationEvaluator** class that you can use to compute this."],"metadata":{}},{"cell_type":"code","source":["evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\nauc = evaluator.evaluate(prediction)\nprint (\"AUC = \", auc)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":12}],"metadata":{"kernelspec":{"display_name":"PySpark","name":"pysparkkernel","language":""},"language_info":{"mimetype":"text/x-python","pygments_lexer":"python2","name":"pyspark","codemirror_mode":{"version":"2","name":"python"}},"name":"Python Parameter Tuning","notebookId":374219277805995},"nbformat":4,"nbformat_minor":0}
